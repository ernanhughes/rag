{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "TqCukcw9EhXK"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ernanhughes/local-rag/blob/main/LlamaIndex_Workflows_AI_Makerspace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LlamaIndex Workflows - AI Makerspace Event\n",
        "\n",
        "In the follow notebook we'll be looking at LlamaIndex's Workflows!\n",
        "\n",
        "We'll cover:\n",
        "\n",
        "1. Events\n",
        "2. Steps\n",
        "3. Tying them together in a Workflow!\n",
        "\n",
        "The example application we'll be using is a Corrective RAG Workflow, based on the research paper [Corrective Retrieval Augmented Generation](https://arxiv.org/pdf/2401.15884)."
      ],
      "metadata": {
        "id": "VCWaBZICzbU5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation\n",
        "\n",
        "As always, we have some work to do before we can jump straight into the workflows.\n",
        "\n",
        "Let's set-up some boilerplate, add some dependencies, and get ready to rock!"
      ],
      "metadata": {
        "id": "BMGjxLBV25E4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Async Boilerplate:\n",
        "\n",
        "Since \"workflows make async a first-class citizen\", and we're running these examples in a Jupyter Notebook (which is in an active async loop!) we'll need to use the `nest_asyncio` library to ensure we're able to take advantage of the async capabilities of the workflows we're making!"
      ],
      "metadata": {
        "id": "cJnMhhnc0zKt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9sDSuocmarA"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Dependencies:\n",
        "\n",
        "Next, we're going to install our dependencies!\n",
        "\n",
        "We'll want to get the [Taviliy Research Tool](https://llamahub.ai/l/tools/llama-index-tools-tavily-research?from=) which will allow us to do open research as part of our Corrective RAG Implementation (more details on that later).\n",
        "\n",
        "We'll also want to grab our `llama-index-utils-workflow` package which will let us draw all possible paths through the resultant workflow."
      ],
      "metadata": {
        "id": "AdoaMp0B1aCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU llama-index llama-index-tools-tavily-research llama-index-utils-workflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhBEvUDlqkaP",
        "outputId": "6c033047-0f23-42f1-9966-c4e81ff9a2dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/756.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/756.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m747.5/756.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Up API Keys\n",
        "\n",
        "Since we'll be using OpenAI's models to power our workflows today, we'll need to provide our OpenAI API Key.\n",
        "\n",
        "> NOTE: Look [here](https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key) to find your API key."
      ],
      "metadata": {
        "id": "XDvx_Yvg2Dc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1X6x__rDqubr",
        "outputId": "dcd9c63f-3901-4c2c-e348-94a734fdd3fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also be using a Taviliy API Key - which you can obtain [here!](https://app.tavily.com/home)"
      ],
      "metadata": {
        "id": "8bGRHK1I2ZvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Tavily API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX_4Uegwq11e",
        "outputId": "886def51-f515-4b02-84ad-b1b1b6ce3238"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tavily API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing Data\n",
        "\n",
        "We're going to be using the HTML file version of the new [EU AI Act](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A32024R1689) so we can ask some questions about the new act that we have!\n",
        "\n",
        "> NOTE: We've provided this in our `DataRepository`, but you could just as easily replace this data with whatever data you desire!\n",
        "\n",
        "We'll get started by cloning the repository."
      ],
      "metadata": {
        "id": "Mn6lDOZi3Eto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AI-Maker-Space/DataRepository"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwGg12u6q6EM",
        "outputId": "2725662b-88c9-4e1e-e467-18ab57108451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DataRepository'...\n",
            "remote: Enumerating objects: 90, done.\u001b[K\n",
            "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 90 (delta 24), reused 29 (delta 8), pack-reused 8 (from 1)\u001b[K\n",
            "Receiving objects: 100% (90/90), 70.26 MiB | 33.70 MiB/s, done.\n",
            "Resolving deltas: 100% (24/24), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we'll move the desired file into our `data` folder for later!"
      ],
      "metadata": {
        "id": "sruGex0q3fmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!cp -r DataRepository/eu_ai_act.html data/eu_ai_act.html"
      ],
      "metadata": {
        "id": "iKG6Z_7CrRFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steps & Events: LlamaIndex Workflow Event Introduction.\n",
        "\n",
        "`Steps` and `Events` comprise the core building-blocks of LlamaIndex Workflows.\n",
        "\n",
        "In the simplest terms:\n",
        "\n",
        "`Steps`:\n",
        "- `Steps` are units of work, or tasks, in a Workflow. They are typically Python functions decorated by `@step`, marking them as part of the Workflow.\n",
        "- Each `Step` is associated with `Events` as input, and `Events` as outputs.\n",
        "  - A `Step` must take, as input, one or more `Events`\n",
        "  - A `Step` must emit, as output, an `Event`.\n",
        "-`Steps` can be extended to have multiple workers in Workflows where that would be an advantage.\n",
        "- `Steps` can modify shared global context (can be thought of as state) as required.\n",
        "\n",
        "`Events`:\n",
        "- `Events` are data structures that pass information between `Steps`.\n",
        "- `Events` are based on Pydantic Models granting all the typical benefits of type validation, etc.\n",
        "- There are two special `Events` worth listing immediately:\n",
        "  - `StartEvent` - the entry point into the Workflow.\n",
        "  - `StopEvent` - this event stops the execution of the current Workflow.\n",
        "\n",
        "That's a lot of information - so let's see how we could set these events up ourselves.\n",
        "\n",
        "We'll need a sample application to do so - so let's cover the core ideas behind Corrective RAG!"
      ],
      "metadata": {
        "id": "FFbO51B73jNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Corrective RAG Overview:\n",
        "\n",
        "Let's take a look at the image provided from the [Corrective Retrieval Augmented Generation](https://i.imgur.com/14rfVyT.png) paper.\n",
        "\n",
        "\n",
        "![image](https://i.imgur.com/14rfVyT.png)\n",
        "\n",
        "To lay out the idea in basic terms, we have a few steps:\n",
        "\n",
        "1. Obtain a user query, and retrieve `k` relevant documents.\n",
        "2. Use a Retrieval Evaluation process to determine if the Retrieved documents are correct/relevant.\n",
        "  - If the retrieved document(s) are correct, this process also refactors the documents to ensure only the most relevant information is retained.\n",
        "  - If the retrieved documents are ambiguous, they go through the above process, but there are also external documents (obtained through search) added to the final context.\n",
        "  - If the retrieved documents are incorrect, only the external search context is used.\n",
        "3. The appropriate contexts are provided as context for the generation, as is typical in RAG."
      ],
      "metadata": {
        "id": "7IZnFV_u50nj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's think about this process in the Event-Driven framework that is LlamaIndex workflows:\n",
        "\n",
        "1. Step 1: Ingest and Process Data\n",
        "  - FUNCTION:\n",
        "    - Ingests, processes, and creates index to be used in following steps.\n",
        "  - INPUT EVENT(S): `StartEvent`\n",
        "  - OUTPUT EVENT(S): `StopEvent`\n",
        "    - signifies the completion of this step\n",
        "  - RETURNS: Created Index\n",
        "2. Step 2: Prepare the Pipeline for Use\n",
        "  - FUNCTION:\n",
        "    - Modifies Workflow context to include all necessary pipelines for the remainder of the Workflow\n",
        "  - INPUT EVENT(S): `StartEvent`\n",
        "  - OUTPUT EVENT(S): `PrepEvent`\n",
        "    - signifying that the pipeline is prepared and ready for Retrieval\n",
        "    - STATE: None required.\n",
        "3. Step 3: Retrieve Context\n",
        "  - INPUT EVENT(S): `PrepEvent`\n",
        "  - OUTPUT EVENT(S): `RetrieveEvent`\n",
        "    - signifies that context has been retrieved\n",
        "    - STATE: List of Contexts Retrieved\n",
        "4. Step 4: Evaluate Context\n",
        "  - FUNCTION:\n",
        "    - Evaluates the current context as compared to the query to determine if the context is relevant.\n",
        "  - INPUT EVENT(S): `RetrieveEvent`\n",
        "  - OUTPUT EVENT(S): `RelevanceEvalEvent`\n",
        "    - signifies that the context has been evaluated and is ready to have the related contexts extracted\n",
        "    - STATE: List of contexts, and results of evaluation for each context.\n",
        "5. Step 5: Extract Relevant Context\n",
        "  - FUNCTION:\n",
        "    - Extracts relevant context based on the evaluation step.\n",
        "  - INPUT EVENT(S): `RelevanceEvalEvent`\n",
        "  - OUTPUT EVENT(S): `TextExtractEvent`\n",
        "    - signifies that the relevant context has been extracted based on the evaluation\n",
        "    - STATE: `str` containing the relevant contexts separated by a `\\n`.\n",
        "6. Step 6: Transform the Query\n",
        "  - FUNCTION:\n",
        "    - Transforms the initial user query to be compatible with the external search tool, and searches if any of the previous contexts were evaluated to be not relevant\n",
        "  - INPUT EVENT(S): `TextExtractEvent`\n",
        "  - OUTPUT EVENT(S): `QueryEvent`\n",
        "    - signifies that the query has been transformed, and the external search tool has been used, and the new context pool is ready\n",
        "    - STATE: `str`: relevant texts, `str`: external searched documents\n",
        "7. Step 7: Final Query\n",
        "  - FUNCTION:\n",
        "    - Uses the retrieved context (that was relevant) and the externally obtained context (if any of the retrieved context was irrelevant) to answer the user's query\n",
        "  - INPUT EVENT(S): `QueryEvent`\n",
        "  - OUTPUT EVENT(S): `StopEvent`\n",
        "    - we're all done!\n",
        "    - STATE: `str` result!"
      ],
      "metadata": {
        "id": "ZBoYItt8CJYH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize Events:\n",
        "\n",
        "Now that we have a plan - let's set this up in code - we'll start by defining our `Events` that will trigger and be emitted by our `Steps`.\n",
        "\n",
        "First up is the `PrepEvent`!\n",
        "\n",
        "We noted above that it didn't need any state, so let's just pass out if we see this `Event`!\n",
        "> NOTE: Notice that each event subclasses `Event`!"
      ],
      "metadata": {
        "id": "1bTl1VwgCgRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import Event\n",
        "from llama_index.core.schema import NodeWithScore\n",
        "\n",
        "class PrepEvent(Event):\n",
        "    \"\"\"Prep event (prepares for retrieval).\"\"\"\n",
        "    pass"
      ],
      "metadata": {
        "id": "Vvml8N9xCj65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next will be our `RetrieveEvent`, we noted that it should have a list of contexts - and so we initialize it as such."
      ],
      "metadata": {
        "id": "nLDgSmX5CxCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RetrieveEvent(Event):\n",
        "    \"\"\"Retrieve event (gets retrieved nodes).\"\"\"\n",
        "\n",
        "    retrieved_nodes: list[NodeWithScore]"
      ],
      "metadata": {
        "id": "OFxE4yToCx8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll quickly initialize the rest of the `Events`, as their construction is (and should be) relatively simple!"
      ],
      "metadata": {
        "id": "4UYWmfy1DXNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RelevanceEvalEvent(Event):\n",
        "    \"\"\"Relevance evaluation event (gets results of relevance evaluation).\"\"\"\n",
        "    relevant_results: list[str]\n",
        "\n",
        "class TextExtractEvent(Event):\n",
        "    \"\"\"Text extract event. Extracts relevant text and concatenates.\"\"\"\n",
        "    relevant_text: str\n",
        "\n",
        "class QueryEvent(Event):\n",
        "    \"\"\"Query event. Queries given relevant text and search text.\"\"\"\n",
        "    relevant_text: str\n",
        "    search_text: str"
      ],
      "metadata": {
        "id": "NCYrJzaKDXsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Templates:\n",
        "\n",
        "As always, when using an LLM Appliation, we'll want to create some prompt templates that achieve our tasks.\n",
        "\n",
        "We'll need two for this application:\n",
        "\n",
        "- `DEFAULT_RELEVANCY_PROMPT_TEMPLATE` - this will be used to evaluate our context to ensure it's relevant to the user's query.\n",
        "- `DEFAULT_TRANSFORM_QUERY_TEMPLATE` - this prompt will transform the user's query to be compatible with our external search tool"
      ],
      "metadata": {
        "id": "_rOmYv9PDjRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "DEFAULT_RELEVANCY_PROMPT_TEMPLATE = PromptTemplate(\n",
        "    template=\"\"\"As a grader, your task is to evaluate the relevance of a document retrieved in response to a user's question.\n",
        "\n",
        "    Retrieved Document:\n",
        "    -------------------\n",
        "    {context_str}\n",
        "\n",
        "    User Question:\n",
        "    --------------\n",
        "    {query_str}\n",
        "\n",
        "    Evaluation Criteria:\n",
        "    - Consider whether the document contains keywords or topics related to the user's question.\n",
        "    - The evaluation should not be overly stringent; the primary objective is to identify and filter out clearly irrelevant retrievals.\n",
        "\n",
        "    Decision:\n",
        "    - Assign a binary score to indicate the document's relevance.\n",
        "    - Use 'yes' if the document is relevant to the question, or 'no' if it is not.\n",
        "\n",
        "    Please provide your binary score ('yes' or 'no') below to indicate the document's relevance to the user question.\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "jfW-yOVurhlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_TRANSFORM_QUERY_TEMPLATE = PromptTemplate(\n",
        "    template=\"\"\"Your task is to refine a query to ensure it is highly effective for retrieving relevant search results. \\n\n",
        "    Analyze the given input to grasp the core semantic intent or meaning. \\n\n",
        "    Original Query:\n",
        "    \\n ------- \\n\n",
        "    {query_str}\n",
        "    \\n ------- \\n\n",
        "    Your goal is to rephrase or enhance this query to improve its search performance. Ensure the revised query is concise and directly aligned with the intended search objective. \\n\n",
        "    Respond with the optimized query only:\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "yv6gzEOArnbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up Steps\n",
        "\n",
        "Next, we'll define our `Steps`!\n",
        "\n",
        "Remember: A `Step` must be triggered by one or more `Events`, and it must emit an `Event`.\n",
        "\n",
        "To get started with our Workflow, we'll need to define a Workflow class.\n",
        "\n",
        "Let's do that!"
      ],
      "metadata": {
        "id": "r5xMJGaNEGY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Workflow Class\n",
        "\n",
        "First things first, we need to create a new class that subclasses `Workflow`.\n",
        "\n",
        "```python\n",
        "class CorrectiveRAGWorkflow(Workflow):\n",
        "```\n",
        "\n",
        "Each step, now, is a method (decorated by the `@step` decorator) which will take an `Event` and `Context` as input."
      ],
      "metadata": {
        "id": "SGtezXbWFArG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### An Aside on Context:\n",
        "\n",
        "`Context`, in workflows, is analagous to `State` in frameworks like LangGraph.\n",
        "\n",
        "It's a way to provide information to multiple `Steps`, without needing to constantly carry forward information in each `Event`.\n",
        "\n"
      ],
      "metadata": {
        "id": "LJrA1T2qE5_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Ingest Pipeline\n",
        "\n",
        "The first `Step` we're going to create is the ingestion step:\n",
        "\n",
        "```python\n",
        "@step\n",
        "async def ingest(self, ctx: Context, ev: StartEvent) -> StopEvent | None:\n",
        "    \"\"\"Ingest step (for ingesting docs and initializing index).\"\"\"\n",
        "    documents: list[Document] | None = ev.get(\"documents\")\n",
        "\n",
        "    if documents is None:\n",
        "        return None\n",
        "\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "    return StopEvent(result=index)\n",
        "```\n",
        "\n",
        "This `Step` simply creates and returns a `VectorStoreIndex` from our provided documents.\n",
        "\n",
        "Notice how we're addresing our `Event` through `ev.get` - this is how we collect results from our `Events` (and `Context`)."
      ],
      "metadata": {
        "id": "L0Q0QorcFl41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2:\n",
        "\n",
        "Next, we need to create a number of pipelines that can be leveraged by our downstream steps.\n",
        "\n",
        "We're going to store a number of these in our `Context` so we don't need to track them through our `Events`.\n",
        "\n",
        "```python\n",
        "@step\n",
        "async def prepare_for_retrieval(\n",
        "    self, ctx: Context, ev: StartEvent\n",
        ") -> PrepEvent | None:\n",
        "    \"\"\"Prepare for retrieval.\"\"\"\n",
        "\n",
        "    query_str: str | None = ev.get(\"query_str\")\n",
        "    retriever_kwargs: dict | None = ev.get(\"retriever_kwargs\", {})\n",
        "\n",
        "    if query_str is None:\n",
        "        return None\n",
        "\n",
        "    tavily_ai_apikey: str | None = ev.get(\"tavily_ai_apikey\")\n",
        "    index = ev.get(\"index\")\n",
        "\n",
        "    llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "    await ctx.set(\"relevancy_pipeline\", QueryPipeline(\n",
        "        chain=[DEFAULT_RELEVANCY_PROMPT_TEMPLATE, llm]\n",
        "    ))\n",
        "    await ctx.set(\"transform_query_pipeline\", QueryPipeline(\n",
        "        chain=[DEFAULT_TRANSFORM_QUERY_TEMPLATE, llm]\n",
        "    ))\n",
        "\n",
        "    await ctx.set(\"llm\", llm)\n",
        "    await ctx.set(\"index\", index)\n",
        "    await ctx.set(\"tavily_tool\", TavilyToolSpec(api_key=tavily_ai_apikey))\n",
        "\n",
        "    await ctx.set(\"query_str\", query_str)\n",
        "    await ctx.set(\"retriever_kwargs\", retriever_kwargs)\n",
        "\n",
        "    return PrepEvent()\n",
        "```\n",
        "\n",
        "As you can see, we're basically using the `Step` to fill up our `Context` with all the relevant tools needed to complete the pipeline."
      ],
      "metadata": {
        "id": "pk2hAkkLGBkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Retrieving Context\n",
        "\n",
        "Next, we build a `Step` that retieves context from our `VectorStoreIndex`.\n",
        "\n",
        "```python\n",
        "@step\n",
        "async def retrieve(\n",
        "    self, ctx: Context, ev: PrepEvent\n",
        ") -> RetrieveEvent | None:\n",
        "    \"\"\"Retrieve the relevant nodes for the query.\"\"\"\n",
        "    query_str = await ctx.get(\"query_str\")\n",
        "    retriever_kwargs = await ctx.get(\"retriever_kwargs\")\n",
        "\n",
        "    if query_str is None:\n",
        "        return None\n",
        "\n",
        "    index = await ctx.get(\"index\", default=None)\n",
        "    tavily_tool = await ctx.get(\"tavily_tool\", default=None)\n",
        "    if not (index or tavily_tool):\n",
        "        raise ValueError(\n",
        "            \"Index and tavily tool must be constructed. Run with 'documents' and 'tavily_ai_apikey' params first.\"\n",
        "        )\n",
        "\n",
        "    retriever: BaseRetriever = index.as_retriever(\n",
        "        **retriever_kwargs\n",
        "    )\n",
        "    result = retriever.retrieve(query_str)\n",
        "    await ctx.set(\"retrieved_nodes\", result)\n",
        "    await ctx.set(\"query_str\", query_str)\n",
        "    return RetrieveEvent(retrieved_nodes=result)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "OR-4dxw6GhXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4:\n",
        "\n",
        "Next, we'll create a `Step` that we can use to evaluate our retrieved documents.\n",
        "\n",
        "```python\n",
        "@step\n",
        "async def eval_relevance(\n",
        "    self, ctx: Context, ev: RetrieveEvent\n",
        ") -> RelevanceEvalEvent:\n",
        "    \"\"\"Evaluate relevancy of retrieved documents with the query.\"\"\"\n",
        "    retrieved_nodes = ev.retrieved_nodes\n",
        "    query_str = await ctx.get(\"query_str\")\n",
        "\n",
        "    relevancy_results = []\n",
        "    for node in retrieved_nodes:\n",
        "        relevancy_pipeline = await ctx.get(\"relevancy_pipeline\")\n",
        "        relevancy = relevancy_pipeline.run(\n",
        "            context_str=node.text, query_str=query_str\n",
        "        )\n",
        "        relevancy_results.append(relevancy.message.content.lower().strip())\n",
        "\n",
        "    await ctx.set(\"relevancy_results\", relevancy_results)\n",
        "    return RelevanceEvalEvent(relevant_results=relevancy_results)\n",
        "```"
      ],
      "metadata": {
        "id": "r3P74-rIG4pS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Extract Relevant Texts\n",
        "\n",
        "Next, we'll extract the contexts that we scored as relevant!\n",
        "\n",
        "```python\n",
        "@step\n",
        "async def extract_relevant_texts(\n",
        "    self, ctx: Context, ev: RelevanceEvalEvent\n",
        ") -> TextExtractEvent:\n",
        "    \"\"\"Extract relevant texts from retrieved documents.\"\"\"\n",
        "    retrieved_nodes = await ctx.get(\"retrieved_nodes\")\n",
        "    relevancy_results = ev.relevant_results\n",
        "\n",
        "    relevant_texts = [\n",
        "        retrieved_nodes[i].text\n",
        "        for i, result in enumerate(relevancy_results)\n",
        "        if result == \"yes\"\n",
        "    ]\n",
        "\n",
        "    result = \"\\n\".join(relevant_texts)\n",
        "    return TextExtractEvent(relevant_text=result)\n",
        "```"
      ],
      "metadata": {
        "id": "BfgxUjwYHFNn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Transform Query\n",
        "\n",
        "Next, we'll (if there are any non-relevant contexts) transform our user's query and query our external search tool.\n",
        "\n",
        "```python\n",
        "    @step\n",
        "    async def transform_query_pipeline(\n",
        "        self, ctx: Context, ev: TextExtractEvent\n",
        "    ) -> QueryEvent:\n",
        "        \"\"\"Search the transformed query with Tavily API.\"\"\"\n",
        "        relevant_text = ev.relevant_text\n",
        "        relevancy_results = await ctx.get(\"relevancy_results\")\n",
        "        query_str = await ctx.get(\"query_str\")\n",
        "\n",
        "        # If any document is found irrelevant, transform the query string for better search results.\n",
        "        if \"no\" in relevancy_results:\n",
        "            qp = await ctx.get(\"transform_query_pipeline\")\n",
        "            transformed_query_str = (qp.run(query_str=query_str).message.content)\n",
        "            # Conduct a search with the transformed query string and collect the results.\n",
        "            search_tool = await ctx.get(\"tavily_tool\")\n",
        "            search_results = search_tool.search(\n",
        "                transformed_query_str, max_results=5\n",
        "            )\n",
        "            search_text = \"\\n\".join([result.text for result in search_results])\n",
        "        else:\n",
        "            search_text = \"\"\n",
        "\n",
        "        return QueryEvent(relevant_text=relevant_text, search_text=search_text)\n",
        "```"
      ],
      "metadata": {
        "id": "2yDWtJVoHsoI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7:\n",
        "\n",
        "All that's left to do is fire off the final query + context package to our LLM for a response!\n",
        "\n",
        "```python\n",
        "@step\n",
        "async def query_result(self, ctx: Context, ev: QueryEvent) -> StopEvent:\n",
        "    \"\"\"Get result with relevant text.\"\"\"\n",
        "    relevant_text = ev.relevant_text\n",
        "    search_text = ev.search_text\n",
        "    query_str = await ctx.get(\"query_str\")\n",
        "\n",
        "    documents = [Document(text=relevant_text + \"\\n\" + search_text)]\n",
        "    index = SummaryIndex.from_documents(documents)\n",
        "    query_engine = index.as_query_engine()\n",
        "    result = query_engine.query(query_str)\n",
        "    return StopEvent(result=result)\n",
        "```"
      ],
      "metadata": {
        "id": "BdNm8ueUI4KX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CODE FOR WORKFLOW:\n",
        "\n",
        "Execute this cell to construct the overall `CorrectiveRAGWorkflow`!"
      ],
      "metadata": {
        "id": "TqCukcw9EhXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import (\n",
        "    Workflow,\n",
        "    step,\n",
        "    Context,\n",
        "    StartEvent,\n",
        "    StopEvent,\n",
        ")\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    Document,\n",
        "    SummaryIndex,\n",
        ")\n",
        "from llama_index.core.query_pipeline import QueryPipeline\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.tools.tavily_research.base import TavilyToolSpec\n",
        "from llama_index.core.base.base_retriever import BaseRetriever\n",
        "\n",
        "class CorrectiveRAGWorkflow(Workflow):\n",
        "    @step\n",
        "    async def ingest(self, ctx: Context, ev: StartEvent) -> StopEvent | None:\n",
        "        \"\"\"Ingest step (for ingesting docs and initializing index).\"\"\"\n",
        "        documents: list[Document] | None = ev.get(\"documents\")\n",
        "\n",
        "        if documents is None:\n",
        "            return None\n",
        "\n",
        "        index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "        return StopEvent(result=index)\n",
        "\n",
        "    @step\n",
        "    async def prepare_for_retrieval(\n",
        "        self, ctx: Context, ev: StartEvent\n",
        "    ) -> PrepEvent | None:\n",
        "        \"\"\"Prepare for retrieval.\"\"\"\n",
        "\n",
        "        query_str: str | None = ev.get(\"query_str\")\n",
        "        retriever_kwargs: dict | None = ev.get(\"retriever_kwargs\", {})\n",
        "\n",
        "        if query_str is None:\n",
        "            return None\n",
        "\n",
        "        tavily_ai_apikey: str | None = ev.get(\"tavily_ai_apikey\")\n",
        "        index = ev.get(\"index\")\n",
        "\n",
        "        llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "        await ctx.set(\"relevancy_pipeline\", QueryPipeline(\n",
        "            chain=[DEFAULT_RELEVANCY_PROMPT_TEMPLATE, llm]\n",
        "        ))\n",
        "        await ctx.set(\"transform_query_pipeline\", QueryPipeline(\n",
        "            chain=[DEFAULT_TRANSFORM_QUERY_TEMPLATE, llm]\n",
        "        ))\n",
        "\n",
        "        await ctx.set(\"llm\", llm)\n",
        "        await ctx.set(\"index\", index)\n",
        "        await ctx.set(\"tavily_tool\", TavilyToolSpec(api_key=tavily_ai_apikey))\n",
        "\n",
        "        await ctx.set(\"query_str\", query_str)\n",
        "        await ctx.set(\"retriever_kwargs\", retriever_kwargs)\n",
        "\n",
        "        return PrepEvent()\n",
        "\n",
        "    @step\n",
        "    async def retrieve(\n",
        "        self, ctx: Context, ev: PrepEvent\n",
        "    ) -> RetrieveEvent | None:\n",
        "        \"\"\"Retrieve the relevant nodes for the query.\"\"\"\n",
        "        query_str = await ctx.get(\"query_str\")\n",
        "        retriever_kwargs = await ctx.get(\"retriever_kwargs\")\n",
        "\n",
        "        if query_str is None:\n",
        "            return None\n",
        "\n",
        "        index = await ctx.get(\"index\", default=None)\n",
        "        tavily_tool = await ctx.get(\"tavily_tool\", default=None)\n",
        "        if not (index or tavily_tool):\n",
        "            raise ValueError(\n",
        "                \"Index and tavily tool must be constructed. Run with 'documents' and 'tavily_ai_apikey' params first.\"\n",
        "            )\n",
        "\n",
        "        retriever: BaseRetriever = index.as_retriever(\n",
        "            **retriever_kwargs\n",
        "        )\n",
        "        result = retriever.retrieve(query_str)\n",
        "        await ctx.set(\"retrieved_nodes\", result)\n",
        "        await ctx.set(\"query_str\", query_str)\n",
        "        return RetrieveEvent(retrieved_nodes=result)\n",
        "\n",
        "    @step\n",
        "    async def eval_relevance(\n",
        "        self, ctx: Context, ev: RetrieveEvent\n",
        "    ) -> RelevanceEvalEvent:\n",
        "        \"\"\"Evaluate relevancy of retrieved documents with the query.\"\"\"\n",
        "        retrieved_nodes = ev.retrieved_nodes\n",
        "        query_str = await ctx.get(\"query_str\")\n",
        "\n",
        "        relevancy_results = []\n",
        "        for node in retrieved_nodes:\n",
        "            relevancy_pipeline = await ctx.get(\"relevancy_pipeline\")\n",
        "            relevancy = relevancy_pipeline.run(\n",
        "                context_str=node.text, query_str=query_str\n",
        "            )\n",
        "            relevancy_results.append(relevancy.message.content.lower().strip())\n",
        "\n",
        "        await ctx.set(\"relevancy_results\", relevancy_results)\n",
        "        return RelevanceEvalEvent(relevant_results=relevancy_results)\n",
        "\n",
        "    @step\n",
        "    async def extract_relevant_texts(\n",
        "        self, ctx: Context, ev: RelevanceEvalEvent\n",
        "    ) -> TextExtractEvent:\n",
        "        \"\"\"Extract relevant texts from retrieved documents.\"\"\"\n",
        "        retrieved_nodes = await ctx.get(\"retrieved_nodes\")\n",
        "        relevancy_results = ev.relevant_results\n",
        "\n",
        "        relevant_texts = [\n",
        "            retrieved_nodes[i].text\n",
        "            for i, result in enumerate(relevancy_results)\n",
        "            if result == \"yes\"\n",
        "        ]\n",
        "\n",
        "        result = \"\\n\".join(relevant_texts)\n",
        "        return TextExtractEvent(relevant_text=result)\n",
        "\n",
        "    @step\n",
        "    async def transform_query_pipeline(\n",
        "        self, ctx: Context, ev: TextExtractEvent\n",
        "    ) -> QueryEvent:\n",
        "        \"\"\"Search the transformed query with Tavily API.\"\"\"\n",
        "        relevant_text = ev.relevant_text\n",
        "        relevancy_results = await ctx.get(\"relevancy_results\")\n",
        "        query_str = await ctx.get(\"query_str\")\n",
        "\n",
        "        # If any document is found irrelevant, transform the query string for better search results.\n",
        "        if \"no\" in relevancy_results:\n",
        "            qp = await ctx.get(\"transform_query_pipeline\")\n",
        "            transformed_query_str = (qp.run(query_str=query_str).message.content)\n",
        "            # Conduct a search with the transformed query string and collect the results.\n",
        "            search_tool = await ctx.get(\"tavily_tool\")\n",
        "            search_results = search_tool.search(\n",
        "                transformed_query_str, max_results=5\n",
        "            )\n",
        "            search_text = \"\\n\".join([result.text for result in search_results])\n",
        "        else:\n",
        "            search_text = \"\"\n",
        "\n",
        "        return QueryEvent(relevant_text=relevant_text, search_text=search_text)\n",
        "\n",
        "    @step\n",
        "    async def query_result(self, ctx: Context, ev: QueryEvent) -> StopEvent:\n",
        "        \"\"\"Get result with relevant text.\"\"\"\n",
        "        relevant_text = ev.relevant_text\n",
        "        search_text = ev.search_text\n",
        "        query_str = await ctx.get(\"query_str\")\n",
        "\n",
        "        documents = [Document(text=relevant_text + \"\\n\" + search_text)]\n",
        "        index = SummaryIndex.from_documents(documents)\n",
        "        query_engine = index.as_query_engine()\n",
        "        result = query_engine.query(query_str)\n",
        "        return StopEvent(result=result)"
      ],
      "metadata": {
        "id": "JE1HHSVRrxHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## \"Graphing\" our Workflow\n",
        "\n",
        "Since we have `Steps` that take `Events` and return `Events` - we can trace through all possible paths and wind up with a graph!"
      ],
      "metadata": {
        "id": "xKrjNugGEk46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.utils.workflow import draw_all_possible_flows\n",
        "\n",
        "draw_all_possible_flows(\n",
        "    CorrectiveRAGWorkflow, filename=\"crag_workflow.html\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhccJokDxVcG",
        "outputId": "0e899c46-3cb4-47df-ea7f-84ccfe97275f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "crag_workflow.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using our Workflow\n",
        "\n",
        "First, we need to set-up our documents, then initialize our Index!"
      ],
      "metadata": {
        "id": "tg5Jn9HNJvhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
        "workflow = CorrectiveRAGWorkflow()\n",
        "index = await workflow.run(documents=documents)"
      ],
      "metadata": {
        "id": "p1o4NJL0r5Ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we're ready to query our Workflow!"
      ],
      "metadata": {
        "id": "llCp8wAlJztZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "response = await workflow.run(\n",
        "    query_str=\"How many parameters make a 'Large Language Model'?\",\n",
        "    index=index,\n",
        "    tavily_ai_apikey=os.environ[\"TAVILY_API_KEY\"],\n",
        ")\n",
        "display(Markdown(str(response)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "j0J5A50Br8bu",
        "outputId": "863afabb-4421-4218-a133-8e97c0e3d253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A Large Language Model typically has at least a billion parameters."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = await workflow.run(\n",
        "    query_str=\"Why does the EU want to regulate AI?\",\n",
        "    index=index,\n",
        "    tavily_ai_apikey=os.environ[\"TAVILY_API_KEY\"],\n",
        ")\n",
        "display(Markdown(str(response)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "UAYlvSTqzAN8",
        "outputId": "5f1a557e-47fd-4925-80b9-ae798f4f4338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The EU wants to regulate AI to improve the functioning of the internal market, promote the uptake of human-centric and trustworthy artificial intelligence, ensure a high level of protection of health, safety, fundamental rights, democracy, the rule of law, and environmental protection, protect against the harmful effects of AI systems, support innovation, prevent fragmentation of the internal market, ensure legal certainty for operators, and facilitate the free circulation, innovation, deployment, and uptake of AI systems within the internal market."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = await workflow.run(\n",
        "    query_str=\"Who owns the IP when it comes to data used to train models?\",\n",
        "    index=index,\n",
        "    tavily_ai_apikey=os.environ[\"TAVILY_API_KEY\"],\n",
        ")\n",
        "display(Markdown(str(response)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "g0NjIPWQJ_Uk",
        "outputId": "0589b083-acae-4ddb-bfa2-cc1b9e667b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Content creators are recognized as the owners of the intellectual property when it comes to the data used to train models."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = await workflow.run(\n",
        "    query_str=\"What are the points of contention between the EU and the US?\",\n",
        "    index=index,\n",
        "    tavily_ai_apikey=os.environ[\"TAVILY_API_KEY\"],\n",
        ")\n",
        "display(Markdown(str(response)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "f8Ndf3_yKIqx",
        "outputId": "5617386f-f481-4511-9969-5933c3317f26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The points of contention between the EU and the US include differing views on European integration, with the US under the current president being seen as undermining rather than encouraging it. Additionally, there is disagreement on the perception of the European Union as a threat versus an ally, as well as injecting conditionality and uncertainty into NATO. Domestic debates in both the US and the EU regarding their respective roles in the world also highlight the fragility of unity between the transatlantic partners."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = await workflow.run(\n",
        "    query_str=\"What AI positions do the EU and US agree on?\",\n",
        "    index=index,\n",
        "    tavily_ai_apikey=os.environ[\"TAVILY_API_KEY\"],\n",
        ")\n",
        "display(Markdown(str(response)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "jEyvgrbAKKBN",
        "outputId": "13446e8b-4c09-4d14-8b53-2912246c5187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The EU and US agree on a risk-based approach to artificial intelligence (AI), prioritizing human safety, ensuring transparency, and supporting safe and trustworthy AI. They also share common fair competition principles on foundational AI models."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> NOTE: This example is adapted from the LlamaIndex [Corrective RAG Workflow Example](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/workflow/corrective_rag_pack.ipynb)."
      ],
      "metadata": {
        "id": "8T61z03Lz8l8"
      }
    }
  ]
}